# -*- coding: utf-8 -*-
"""Hyperparameter search Visualize_nn_imagination.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCbniTGqM8VKGtmwYy2hT39ixzppg2Zl

Implements experirments to visualize environment states a network is representing. See [this doc](https://docs.google.com/document/d/1fzUuBwzbUfme8k8BmRtAel7LlzJSA6dxcc7bPXJIwBg/edit?usp=sharing) for more details.
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Imports
 
import pandas as pd
import numpy as np
import sys
from scipy.signal import convolve2d
from matplotlib import pyplot as plt
import random
 
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
print("tf version: %s" % tf.__version__)
# %matplotlib inline
!pip install ray[tune]

"""#Training data functions"""

def life_step(X):
    nbrs_count = convolve2d(X, np.ones((3, 3)), mode='same', boundary='fill') - X
    return (nbrs_count == 3) | (X & (nbrs_count == 2))

board_size = (20, 20)

def convert_model_in(data):
  data = np.array(data)
  data = data.astype(int)
  data = np.expand_dims(data, -1)
  return data

def gen_data_batch(size, skip):
  datas = []
  start_next = None
  for _ in range(size):
    if np.random.rand(1) < .1 or start_next is None:
      life_state = np.random.rand(board_size[0], board_size[1]) > .5
    else:
      life_state = start_next

    data = []
    data.append(life_state)
    for i in range(skip):
      life_state = life_step(life_state)
      data.append(life_state)
      if i == 0:
        start_next = life_state
    datas.append(data)

  datas = convert_model_in(datas)

  return datas

"""# Model and training."""

input_shape = board_size+(1,)
input = tf.keras.Input(shape=input_shape)
loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0)
leak_relu = tf.keras.layers.LeakyReLU

def create_models(encoded_size, T, use_residual):

  encoder = tf.keras.Sequential(
      [
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
      ], name="encoder",
  )
  intermediates = [encoder(input)]
  all_hidden = []

  timestep_model = tf.keras.Sequential(
      [
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1), input_shape=board_size+(encoded_size,)),
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
      ], name="timestep_model",
  )
  timestep_model_all_hidden_out = [layer.output for layer in timestep_model.layers]
  timestep_model_all_hidden_out[-1] += timestep_model.layers[0].input
  timestep_model_all_hidden = tf.keras.Model(timestep_model.inputs, timestep_model_all_hidden_out)
  for i in range(T):
    timestep_all_hidden = timestep_model_all_hidden(intermediates[-1])
    if use_residual:
      timestep_all_hidden[-1] += intermediates[-1]
    intermediates.append(timestep_all_hidden[-1])
    all_hidden.append(timestep_all_hidden)

  decoder = tf.keras.Sequential(
      [
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
        tf.keras.layers.Conv2D(1, 3, activation=None, padding='same', kernel_regularizer=tf.keras.regularizers.l2(1)),
      ], name="decoder",
  )

  model = tf.keras.Model(inputs=input, outputs=intermediates)
  all_hidden_model = tf.keras.Model(inputs=input, outputs=all_hidden)

  discriminator = tf.keras.Sequential(
      [
        tf.keras.layers.Conv2D(4, 3, strides=2, activation=leak_relu()),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Conv2D(8, 3, strides=2, activation=leak_relu()),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Conv2D(16, 3, strides=2, activation=leak_relu()),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1),
      ], name="discriminator",
  )

  return encoder, intermediates, all_hidden, decoder, model, all_hidden_model, discriminator

train_acc_metric = tf.keras.metrics.BinaryAccuracy()
non_train_acc_metric = tf.keras.metrics.BinaryAccuracy()
discrim_acc_metric = tf.keras.metrics.BinaryAccuracy()
gen_acc_metric = tf.keras.metrics.BinaryAccuracy()
reg_loss_metric = tf.keras.metrics.Mean()
metrics = [
    ["train_acc", train_acc_metric],
    ["non_train_acc", non_train_acc_metric],
    ["discrim_acc", discrim_acc_metric],
    ["gen_acc", gen_acc_metric],
    ["reg loss", reg_loss_metric],
]

def calc_discriminator_loss(discrim_on_real, discrim_on_gen):
    real_loss = loss_fn(tf.ones_like(discrim_on_real), discrim_on_real)
    discrim_acc_metric.update_state(tf.ones_like(discrim_on_real), discrim_on_real)

    fake_loss = loss_fn(tf.zeros_like(discrim_on_gen), discrim_on_gen)
    discrim_acc_metric.update_state(tf.zeros_like(discrim_on_gen), discrim_on_gen)
    total_loss = real_loss + fake_loss
    return total_loss

def get_batch(datas, batch_size):
    idx = np.random.choice(np.arange(len(datas)), batch_size, replace=False)
    return datas[idx]

def get_train_model(model, discriminator, optimizer, datas, discriminator_opt, T, reg_amount=0):
  @tf.function
  def train_step(batch, adver_batch, decoder, indexes_to_train, indexes_to_adver, train_model):
    inputs_batch = batch[:, 0]
    outputs_batch = batch
    with tf.GradientTape() as tape, tf.GradientTape() as disc_tape:
      generator_loss = 0.0
      discriminator_loss = 0.0
      ran_discrim = False
      model_outputs = model(inputs_batch)

      reg_loss = sum(model.losses) + sum(decoder.losses)
      reg_loss_metric.update_state([reg_loss])
      loss = reg_loss * reg_amount

      for i in range(T+1):
        pred = decoder(model_outputs[i])
        if i in indexes_to_train:
          loss += loss_fn(outputs_batch[:, i], pred)
          train_acc_metric.update_state(outputs_batch[:, i], pred)
        else:
          non_train_acc_metric.update_state(outputs_batch[:, i], pred)

        if i in indexes_to_adver:
          discrim_on_pred = discriminator(tf.math.sigmoid(pred))
          discrim_on_real = discriminator(adver_batch[:,0])
          discriminator_loss += calc_discriminator_loss(discrim_on_real, discrim_on_pred)
          generator_loss = loss_fn(tf.ones_like(discrim_on_pred), discrim_on_pred)
          gen_acc_metric.update_state(tf.ones_like(discrim_on_pred), discrim_on_pred)
          loss += generator_loss
          ran_discrim = True

      loss /= len(indexes_to_train)

    trainable_weights =  decoder.trainable_weights
    if train_model:
      trainable_weights += model.trainable_weights
    grads = tape.gradient(loss, trainable_weights)
    clip_val = .1
    grads = [(tf.clip_by_value(grad, -clip_val, clip_val))
                                      for grad in grads]
    optimizer.apply_gradients(zip(grads, trainable_weights))

    if ran_discrim:
      disctim_grads = disc_tape.gradient(discriminator_loss, discriminator.trainable_weights)
      discriminator_opt.apply_gradients(zip(disctim_grads, discriminator.trainable_weights))
    
  def train_model(decoder, indexes_to_train, indexes_to_adver, train_model, stop_acc):
    for name, metric in metrics:
      metric.reset_states()

    for step_i in range(20000):
      batch = get_batch(datas, 128)
      adver_batch = get_batch(datas, 128)

      train_step(tf.constant(batch), tf.constant(adver_batch), decoder, indexes_to_train, indexes_to_adver, train_model)
      if step_i % 500 == 0:
        for name, metric in metrics:
          print(step_i, name, metric.result().numpy())
        print("=" * 100, flush=True)

        if train_acc_metric.result().numpy() > stop_acc and step_i > 0:
            break

        for name, metric in metrics:
          metric.reset_states()

  return train_model

def np_sig(x):
  return 1/(1 + np.exp(-x)) 


def get_gen_boards(decoder, model_results):
  gen_boards = []
  for i in range(len(model_results)):
    gen_boards.append(decoder(model_results[i]).numpy())

  gen_boards = np.array(gen_boards)
  gen_boards = np_sig(gen_boards)
  gen_boards = np.transpose(gen_boards, (1,0,2,3,4))
  return gen_boards

def single_index_metric(gt, gen, thresh):
  """Returns the porportion of gen instances close enough to the ground truth."""
  equal = np.equal(gt, gen>.5)
  acc = np.mean(equal, (1,2,3))
  over_thresh = acc >= thresh
  return np.mean(over_thresh)

def single_gt_index_metric(gt, gen_boards, thresh, non_train_indexies):
  """Metric representing how well the gen boards represent the single ground truth state.
  
  It gets full credit the time it predicts the state the best, and partial credit for all other times.
  """
  metrics_for_gt = []
  for j in non_train_indexies:
    metrics_for_gt.append(single_index_metric(gt, gen_boards[:, j], thresh))
  metrics_for_gt.sort(reverse = True)
  weights = np.ones([len(non_train_indexies)]) * .5
  weights[0] = 1
  result = sum(np.multiply(metrics_for_gt, weights))
  return result

def metric(eval_datas, gen_boards, thresh, non_train_indexies):
  """Averages the metric on each of the ground truth states."""
  total_metric = 0
  for i in non_train_indexies:
    total_metric += single_gt_index_metric(eval_datas[:, i], gen_boards, thresh, non_train_indexies)
  return total_metric / float(len(non_train_indexies))

def combine_metric(eval_datas, gen_boards, adver_gen_boards, thresh, non_train_indexies):
  adver_metric = metric(eval_datas, adver_gen_boards, thresh, non_train_indexies)
  regular_metric = metric(eval_datas, gen_boards, thresh, non_train_indexies)
  return max(adver_metric, regular_metric)

def everything(config):
  T = config['T']
  encoded_size = config['encoded_size']
  datas = gen_data_batch(100000, T)
  eval_datas = gen_data_batch(10000, T)
  encoder, intermediates, all_hidden, decoder, model, all_hidden_model, discriminator = create_models(encoded_size, T, False)

  optimizer=tf.keras.optimizers.Adam(config['learning_rate']) # .001 lr works better than smaller ones.
  # TODO: Lower learning rate as non train accuracy improves. Might help not mess up the hidden representations that it learned.
  discriminator_opt=tf.keras.optimizers.Adam()

  train_indexies = [0,T]
  get_train_model(model, discriminator, optimizer, datas, discriminator_opt, T, reg_amount=0)(decoder, train_indexies, [], True, .99)

  adver_decoder = tf.keras.Sequential(
      [
        tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x)),
        tf.keras.layers.Conv2D(encoded_size, 3, activation=leak_relu(), padding='same'),
        tf.keras.layers.Conv2D(1, 3, activation=None, padding='same'),
      ], name="adver_decoder",
  )

  get_train_model(model, discriminator, optimizer, datas, discriminator_opt, T, reg_amount=0)(adver_decoder, train_indexies, [], False, .99)
  print("Training adversarally")
  non_train_indexies = range(1, T)
  get_train_model(model, discriminator, optimizer, datas, discriminator_opt, T, reg_amount=0)(adver_decoder, train_indexies, non_train_indexies, False, .99)

  model_results = model(eval_datas[:, 0])
  gen_boards = get_gen_boards(decoder, model_results)
  adver_gen_boards = get_gen_boards(adver_decoder, model_results)

  return combine_metric(eval_datas, gen_boards, adver_gen_boards, .95, non_train_indexies)

everything({
        "learning_rate": 0.001,
        "T": 3,
        "encoded_size": 32,
    })

from ray import tune
analysis = tune.run(
    everything,
    config={
        "learning_rate": tune.grid_search([0.001, 0.01, 0.1]),
        "T": tune.choice([3, 4]),
        "encoded_size": tune.choice([32])
    })

print("Best config: ", analysis.get_best_config(metric="mean_loss"))

# Get a dataframe for analyzing trial results.
df = analysis.dataframe()

metric_test_eval_datas = gen_data_batch(10000, 4)

def test_metric(eval_datas, gen_boards, thresh, expected_min, expected_max):
  for i in range(2):
    print("eval_datas:")
    plt_boards(eval_datas[i])

    print("gen_boards:")
    plt_boards(gen_boards[i])

    print("=" * 40)
  
  metric_val = metric(eval_datas, gen_boards, thresh)

  print("metric_val", metric_val)

  assert(metric_val >= expected_min)
  assert(metric_val <= expected_max)

print("Same data should be 1 ish, even when reordered")
metric_test_gen_datas = np.stack([metric_test_eval_datas[:, 0], metric_test_eval_datas[:, 2], metric_test_eval_datas[:, 3], metric_test_eval_datas[:, 1], metric_test_eval_datas[:, 4]], axis=1)
test_metric(metric_test_eval_datas, metric_test_gen_datas, .99, 1, 1.1)

print("Threshold works")
assert(metric(metric_test_eval_datas, metric_test_gen_datas, .99) < metric(metric_test_eval_datas, metric_test_gen_datas, .4))

print("Only gets partial credit if repeating same state")
metric_test_gen_datas = np.stack([metric_test_eval_datas[:, 0], metric_test_eval_datas[:, 1], metric_test_eval_datas[:, 2], metric_test_eval_datas[:, 2], metric_test_eval_datas[:, 4]], axis=1)
# It should get (1 + 1 + .5) / 3 = .83
test_metric(metric_test_eval_datas, metric_test_gen_datas, .99, .83, .87)

print("No credit for start or end state")
metric_test_gen_datas = np.stack([metric_test_eval_datas[:, 0], metric_test_eval_datas[:, 2], metric_test_eval_datas[:, 0], metric_test_eval_datas[:, 4], metric_test_eval_datas[:, 4]], axis=1)
# It should get (1 + 0 + 0) / 3 = .33
test_metric(metric_test_eval_datas, metric_test_gen_datas, .99, .33, .4)

print("Combine metric works")
combine_val = combine_metric(metric_test_eval_datas, metric_test_eval_datas, metric_test_gen_datas, .95)
print(combine_val)
assert(combine_val >= 1)
assert(combine_val <= 1.1)

"""# Locate representation method"""

def get_loc_rep_decoder():
  return tf.keras.Sequential(
      [
        tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x)),
        tf.keras.layers.Conv2D(1, 3, activation=None, padding='same'),
      ], name="loc_rep_decoder",
  )

def get_loc_rep_model(hidden_model_output):
  loc_rep_decoder = get_loc_rep_decoder()
  loc_rep_model_ouptut = loc_rep_decoder(hidden_model_output)
  loc_rep_model = tf.keras.Model(inputs=input, outputs=loc_rep_model_ouptut)
  loc_rep_model.compile(
      optimizer=tf.keras.optimizers.Adam(.001),
      loss=loss_fn,
      metrics = 'accuracy',
  )
  return loc_rep_model

def eval_loc_rep_models(step):
  all_hidden_model_outputs = np.array(all_hidden_model(input))
  model_accs = np.zeros_like(all_hidden_model_outputs)
  for index, hidden_model_output in np.ndenumerate(all_hidden_model_outputs):
    print("-"*100)
    loc_rep_model = get_loc_rep_model(hidden_model_output)
    early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=1, min_delta=.001, verbose=1)
    loc_rep_model.fit(datas[:, 0], datas[:, step], epochs=100, verbose=1, callbacks=[early_stop])
    model_accs[index] = loc_rep_model.evaluate(eval_datas[:, 0], eval_datas[:, step], verbose=0)[1]
  return model_accs

def eval_loc_rep_models_ave(step):
  all_model_accs = []
  for i in range(1):
    print("="*100)
    all_model_accs.append(eval_loc_rep_models(step))

  return np.mean(all_model_accs, 0)

all_loc_rep_accs = []
for i in range(T):
  all_loc_rep_accs.append(eval_loc_rep_models_ave(i))

all_loc_rep_accs

